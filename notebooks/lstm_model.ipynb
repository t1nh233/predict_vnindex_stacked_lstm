{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t1nh233/predict_vnindex_stacked_lstm/blob/main/notebooks/lstm_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T5NTTcF27HpV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "6d58dd80-7bf9-4637-a8c6-dfe0c8f39c18"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-511721571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m from src.utils import (\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mscale_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_sliding_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import importlib.metadata\n",
        "\n",
        "def setup_environment(file_path=\"requirements.txt\"):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path) as f:\n",
        "            requirements = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
        "    else:\n",
        "        requirements = [\"numpy<2.0.0\", \"pandas_ta\", \"optuna\", \"pandas\", \"torch\", \"matplotlib\", \"scikit-learn\"]\n",
        "\n",
        "    ignore_libs = {\"os\", \"sys\", \"json\", \"math\", \"time\", \"random\", \"datetime\"}\n",
        "\n",
        "    for req in requirements:\n",
        "        pkg_name = req.split(\"==\")[0].split(\"<\")[0].split(\">\")[0]\n",
        "        if pkg_name in ignore_libs:\n",
        "            continue\n",
        "        try:\n",
        "            importlib.metadata.version(pkg_name)\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", req])\n",
        "\n",
        "if os.path.exists(\"predict_vnindex_stacked_lstm\"):\n",
        "    os.chdir(\"predict_vnindex_stacked_lstm\")\n",
        "\n",
        "setup_environment(\"requirements.txt\")\n",
        "\n",
        "import json\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from src.utils import (\n",
        "    load_and_process_data, feature_extraction, split_data,\n",
        "    scale_data, create_sliding_window,\n",
        "    train_model,\n",
        "    validate_model,\n",
        "    predict_model,\n",
        "    inverse_transform_target,\n",
        "    cal_metrics\n",
        ")\n",
        "from src.model import LSTMModel\n",
        "\n",
        "BASE_DIR = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load du lieu\n",
        "url = \"https://raw.githubusercontent.com/t1nh233/predict_vnindex_stacked_lstm/refs/heads/main/data/raw/vn_index_historical_data_9_12.csv\"\n",
        "vnindex = load_and_process_data(url)\n",
        "\n",
        "## Xu ly cac dac trung moi\n",
        "vnindex_feature = feature_extraction(vnindex)\n",
        "\n",
        "## Cac dac trung su dung de huan luyen\n",
        "FEATURE_COLUMNS = ['Close', 'Volume', 'RSI', 'SMA', 'EMA', 'MACD_Hist', 'BB_Width', 'BB_Percentage']\n",
        "TARGET_COLUMN = 'Label'\n",
        "\n",
        "## Trich xuat du lieu theo cac dac trung huan luyen\n",
        "feature_data = vnindex_feature[FEATURE_COLUMNS + [TARGET_COLUMN]].copy()\n",
        "TARGET_INDEX = feature_data.columns.get_loc(TARGET_COLUMN)\n",
        "\n",
        "## Chia tap du lieu train, valid, set theo ti le 70:20:10\n",
        "train_df, val_df, test_df = split_data(feature_data, 0.7, 0.2)\n",
        "\n",
        "## Scale data ve (0, 1)\n",
        "scaler, train_scaled_df, val_scaled_df, test_scaled_df = scale_data(train_df, val_df, test_df, FEATURE_COLUMNS + [TARGET_COLUMN])\n",
        "\n",
        "## Tao input (sliding window) cho LSTM\n",
        "WINDOW_SIZE = 30\n",
        "X_train, y_train = create_sliding_window(train_scaled_df, WINDOW_SIZE)\n",
        "X_val, y_val = create_sliding_window(val_scaled_df, WINDOW_SIZE)\n",
        "X_test, y_test = create_sliding_window(test_scaled_df, WINDOW_SIZE)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)"
      ],
      "metadata": {
        "id": "LsP6Z1OpIFH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_DIR = os.path.join('..', 'models')\n",
        "# RESULT_DIR = os.path.join('..', 'results')\n",
        "# FIGURE_DIR = os.path.join('..', 'figures')\n",
        "\n",
        "# # Tao thu muc neu chua co\n",
        "# os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "# os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "# os.makedirs(FIGURE_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
        "RESULT_DIR = os.path.join(BASE_DIR, 'results')\n",
        "FIGURE_DIR = os.path.join(BASE_DIR, 'figures')\n",
        "\n",
        "# Tạo thư mục (nếu chưa có)\n",
        "for d in [MODEL_DIR, RESULT_DIR, FIGURE_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)"
      ],
      "metadata": {
        "id": "TMo4fGXow6sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dieu chinh tham so cua mo hinh (chon ra bo tham so tot nhat)\n",
        "\n",
        "def hyper_tuning(trial):\n",
        "  ## Sieu tham so cho model\n",
        "  hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
        "  num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "  dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
        "\n",
        "  ## Sieu tham so cho optimizer\n",
        "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
        "  weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  ## Khoi tao model\n",
        "  model = LSTMModel(\n",
        "      input_size = X_train.shape[2],\n",
        "      hidden_size = hidden_size,\n",
        "      num_layers = num_layers,\n",
        "      dropout_rate = dropout_rate\n",
        "  ).to(device)\n",
        "\n",
        "  loss_func = nn.HuberLoss(delta=1.0)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "  ## Training voi epoch 10 tren train de test\n",
        "  for epoch in range(10):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model(X_batch)\n",
        "      loss = loss_func(y_batch, y_pred)\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "  ## Validate\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for X_batch_v, y_batch_v in val_loader:\n",
        "        X_batch_v, y_batch_v = X_batch_v.to(device), y_batch_v.to(device)\n",
        "\n",
        "        y_pred_v = model(X_batch_v)\n",
        "        loss = loss_func(y_batch_v, y_pred_v)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    ## In ra ket qua khi chay tren bo tham so nay\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    trial.report(avg_val_loss, epoch)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return avg_val_loss\n",
        "\n",
        "## Bat dau chay\n",
        "print(\"Start tuning\")\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(hyper_tuning, n_trials=20)\n",
        "\n",
        "## Tra ve bo tham so toi uu nhat\n",
        "print(\"Best parameter: \", study.best_params)\n",
        "best_params = study.best_params\n",
        "\n",
        "## Luu best params vao file\n",
        "best_params_config = {\n",
        "    \"window_size\": WINDOW_SIZE,\n",
        "    \"feature_columns\": FEATURE_COLUMNS,\n",
        "    **best_params\n",
        "}\n",
        "\n",
        "params_path = os.path.join(MODEL_DIR, 'best_params.json')\n",
        "with open(params_path, 'w') as f:\n",
        "    json.dump(best_params_config, f, indent=4)"
      ],
      "metadata": {
        "id": "FfiQNGq1w5f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Huan luyen mo hinh\n",
        "final_batch_size = best_params['batch_size']\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=final_batch_size, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=final_batch_size, shuffle=False)\n",
        "\n",
        "model = LSTMModel(\n",
        "    input_size = X_train.shape[2],\n",
        "    hidden_size = best_params['hidden_size'],\n",
        "    num_layers = best_params['num_layers'],\n",
        "    dropout_rate = best_params['dropout_rate']\n",
        ").to(device)\n",
        "\n",
        "loss_func = nn.HuberLoss(delta=1.0)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "best_val_loss = float('inf')\n",
        "history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "print(\"Start training\")\n",
        "model_save_path = os.path.join(MODEL_DIR, 'best_vnindex_lstm.pth')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  avg_train_loss = train_model(model, train_loader, loss_func, optimizer, device)\n",
        "  avg_val_loss = validate_model(model, val_loader, loss_func, device)\n",
        "\n",
        "  history['train_loss'].append(avg_train_loss)\n",
        "  history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "  if avg_val_loss < best_val_loss:\n",
        "    best_val_loss = avg_val_loss\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.5f}, Val Loss: {avg_val_loss:.5f}')\n",
        "\n",
        "scaler_path = os.path.join(MODEL_DIR, 'scaler.pkl')\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "## Luu lich su huan luyen\n",
        "history_path = os.path.join(RESULT_DIR, 'training_history.csv')\n",
        "pd.DataFrame(history).to_csv(history_path, index=False)\n",
        "\n",
        "## Ve bieu do train_loss va val_loss\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "loss_chart_path = os.path.join(FIGURE_DIR, 'loss_chart.png')\n",
        "plt.savefig(loss_chart_path)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RzAv_b_kNw_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = os.path.join('..', 'models', 'best_vnindex_lstm.pth')\n",
        "\n",
        "# Load trong so da huan luyen vao model\n",
        "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=final_batch_size, shuffle=False)\n",
        "\n",
        "## Du doan tren tap test\n",
        "preds, targets = predict_model(model, test_loader, device)\n",
        "\n",
        "## Inverse lai cac gia tri\n",
        "final_preds = inverse_transform_target(preds, scaler, TARGET_INDEX)\n",
        "final_targets = inverse_transform_target(targets, scaler, TARGET_INDEX)\n",
        "\n",
        "## Tinh metrics\n",
        "mae, rmse, r2 = cal_metrics(final_targets, final_preds)\n",
        "\n",
        "print(f\"MAE: {mae:.2f} điểm\")\n",
        "print(f\"RMSE: {rmse:.2f} điểm\")\n",
        "print(f\"R2 Score: {r2:.4f}\")\n",
        "\n",
        "## Luu ket qua\n",
        "\n",
        "metrics_path = os.path.join(RESULT_DIR, 'evaluation_metrics.txt')\n",
        "with open(metrics_path, 'w') as f:\n",
        "    f.write(f\"MAE: {mae:.4f}\\n\")\n",
        "    f.write(f\"RMSE: {rmse:.4f}\\n\")\n",
        "    f.write(f\"R2 Score: {r2:.4f}\\n\")\n",
        "\n",
        "# Ve bieu do truc quan giua gia tri thuc te va gia tri du doan\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(final_targets, label='Thực tế (VN-Index)', color='blue')\n",
        "plt.plot(final_preds, label='Dự báo (LSTM)', color='red', alpha=0.7)\n",
        "plt.title('So sánh Giá trị thực tế và Giá trị mô hình dự báo trên Test set')\n",
        "plt.legend()\n",
        "\n",
        "pred_chart_path = os.path.join(FIGURE_DIR, 'prediction_chart.png')\n",
        "plt.savefig(pred_chart_path)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8UG4UtnXedIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tai cac file luu param, model, ...\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "targets = ['models', 'results', 'figures']\n",
        "found_paths = []\n",
        "\n",
        "for target in targets:\n",
        "    if os.path.exists(target):\n",
        "        found_paths.append(target)\n",
        "    elif os.path.exists(os.path.join('..', target)):\n",
        "        found_paths.append(os.path.join('..', target))\n",
        "\n",
        "if found_paths:\n",
        "    paths_str = \" \".join(found_paths)\n",
        "    exit_code = os.system(f\"zip -r training_results.zip {paths_str}\")\n",
        "    if exit_code == 0:\n",
        "        files.download('training_results.zip')"
      ],
      "metadata": {
        "id": "Hcf-kE9Pt8O6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}